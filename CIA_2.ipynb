{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Optimizer: A Reinforcement Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Grid Environment with obstacles\n",
    "class GridEnvironment:\n",
    "    def __init__(self, size=100, obstacle_prob=0.2):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size))\n",
    "        self.start = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        self.goal = (random.randint(0, size-1), random.randint(0, size-1))\n",
    "        \n",
    "        # Set obstacles\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                if random.random() < obstacle_prob and (i, j) != self.start and (i, j) != self.goal:\n",
    "                    self.grid[i, j] = -1  # Represent obstacles by -1\n",
    "\n",
    "    def is_obstacle(self, state):\n",
    "        return self.grid[state] == -1\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        return state == self.goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define MDP Transition and Reward Functions\n",
    "class MDP:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "    \n",
    "    def transition(self, state, action):\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if 0 <= next_state[0] < self.env.size and 0 <= next_state[1] < self.env.size:\n",
    "            if not self.env.is_obstacle(next_state):\n",
    "                return next_state\n",
    "        return state  # Stay in place if hitting an obstacle or boundary\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        return 1 if next_state == self.env.goal else -0.1  # Small penalty for each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement Policy Iteration Agent\n",
    "class PolicyIterationAgent:\n",
    "    def __init__(self, mdp, gamma=0.9, theta=1e-3):\n",
    "        self.mdp = mdp\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.value_table = np.zeros((self.mdp.env.size, self.mdp.env.size))\n",
    "        self.policy = np.random.choice(range(len(self.mdp.actions)), (self.mdp.env.size, self.mdp.env.size))\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for i in range(self.mdp.env.size):\n",
    "                for j in range(self.mdp.env.size):\n",
    "                    state = (i, j)\n",
    "                    if self.mdp.env.is_terminal(state) or self.mdp.env.is_obstacle(state):\n",
    "                        continue\n",
    "                    \n",
    "                    action = self.mdp.actions[self.policy[i, j]]\n",
    "                    next_state = self.mdp.transition(state, action)\n",
    "                    reward = self.mdp.reward(state, action, next_state)\n",
    "                    \n",
    "                    v = self.value_table[i, j]\n",
    "                    self.value_table[i, j] = reward + self.gamma * self.value_table[next_state]\n",
    "                    delta = max(delta, abs(v - self.value_table[i, j]))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for i in range(self.mdp.env.size):\n",
    "            for j in range(self.mdp.env.size):\n",
    "                state = (i, j)\n",
    "                if self.mdp.env.is_terminal(state) or self.mdp.env.is_obstacle(state):\n",
    "                    continue\n",
    "                \n",
    "                old_action = self.policy[i, j]\n",
    "                action_values = []\n",
    "                for action in self.mdp.actions:\n",
    "                    next_state = self.mdp.transition(state, action)\n",
    "                    reward = self.mdp.reward(state, action, next_state)\n",
    "                    action_values.append(reward + self.gamma * self.value_table[next_state])\n",
    "                \n",
    "                self.policy[i, j] = np.argmax(action_values)\n",
    "                if old_action != self.policy[i, j]:\n",
    "                    policy_stable = False\n",
    "        return policy_stable\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        while True:\n",
    "            self.policy_evaluation()\n",
    "            if self.policy_improvement():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Implement Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, mdp, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=1000):\n",
    "        self.mdp = mdp\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.q_table = np.zeros((self.mdp.env.size, self.mdp.env.size, len(self.mdp.actions)))\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.mdp.env.start\n",
    "            while not self.mdp.env.is_terminal(state):\n",
    "                if random.uniform(0, 1) < self.epsilon:\n",
    "                    action_idx = random.randint(0, len(self.mdp.actions) - 1)\n",
    "                else:\n",
    "                    action_idx = np.argmax(self.q_table[state[0], state[1]])\n",
    "                \n",
    "                action = self.mdp.actions[action_idx]\n",
    "                next_state = self.mdp.transition(state, action)\n",
    "                reward = self.mdp.reward(state, action, next_state)\n",
    "                \n",
    "                best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
    "                td_target = reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action]\n",
    "                td_delta = td_target - self.q_table[state[0], state[1], action_idx]\n",
    "                self.q_table[state[0], state[1], action_idx] += self.alpha * td_delta\n",
    "                \n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Value Table: [[-0.99997942 -0.99997713 -0.99997459 ... -0.99999936 -0.99999943\n",
      "  -0.99999948]\n",
      " [ 0.         -0.99997459 -0.99997177 ... -0.99999929 -0.99999936\n",
      "  -0.99999943]\n",
      " [-0.99997459 -0.99997177 -0.99996863 ... -0.99999921 -0.99999929\n",
      "  -0.99999936]\n",
      " ...\n",
      " [-0.98969245 -0.98854717 -0.98727463 ... -0.99979101 -0.99981191\n",
      "  -0.99983072]\n",
      " [-0.9907232  -0.98969245 -0.98854717 ... -0.99981191 -0.99983072\n",
      "  -0.99984765]\n",
      " [-0.99165088 -0.9907232  -0.98969245 ... -0.99983072 -0.99984765\n",
      "  -0.99986288]]\n",
      "Q-Learning Q-Table: [[[-0.44653411 -0.44592607 -0.44479777 -0.4414602 ]\n",
      "  [-0.44640663 -0.44816793 -0.44332749 -0.44618465]\n",
      "  [-0.44694058 -0.44601725 -0.44923204 -0.44578683]\n",
      "  ...\n",
      "  [-0.41917654 -0.4180863  -0.42200271 -0.41775029]\n",
      "  [-0.41895861 -0.41832715 -0.42159233 -0.41692624]\n",
      "  [-0.41685579 -0.41912154 -0.42221073 -0.41816637]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [-0.44614612 -0.44859219 -0.44695701 -0.44444413]\n",
      "  [-0.44900991 -0.44529491 -0.44552132 -0.44799942]\n",
      "  ...\n",
      "  [-0.41715352 -0.41472834 -0.41866198 -0.41907785]\n",
      "  [-0.42149774 -0.4175657  -0.41851343 -0.41651957]\n",
      "  [-0.41840163 -0.4197025  -0.41728096 -0.42196315]]\n",
      "\n",
      " [[-0.44631047 -0.44740051 -0.44626353 -0.44676603]\n",
      "  [-0.44638292 -0.4471681  -0.4477725  -0.44454287]\n",
      "  [-0.44508623 -0.44827081 -0.44515793 -0.44775506]\n",
      "  ...\n",
      "  [-0.41709706 -0.41378043 -0.4151198  -0.41339919]\n",
      "  [-0.41322811 -0.41120593 -0.41441231 -0.4222951 ]\n",
      "  [-0.41603616 -0.41801428 -0.41440698 -0.41728973]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.65742452 -0.65525259 -0.6545576  -0.65586253]\n",
      "  [-0.65894277 -0.65604075 -0.65708105 -0.65752759]\n",
      "  [-0.66036101 -0.65601022 -0.65837036 -0.65800332]\n",
      "  ...\n",
      "  [-0.42194524 -0.41934688 -0.42027667 -0.4174003 ]\n",
      "  [-0.42155367 -0.42467437 -0.41956152 -0.42294208]\n",
      "  [-0.41763501 -0.41946342 -0.42198132 -0.41940258]]\n",
      "\n",
      " [[-0.65720494 -0.65555392 -0.65464906 -0.65403608]\n",
      "  [-0.65785521 -0.65478353 -0.65647021 -0.65669681]\n",
      "  [-0.65790832 -0.6579241  -0.65550816 -0.65845829]\n",
      "  ...\n",
      "  [-0.41860876 -0.41828188 -0.42295295 -0.42038618]\n",
      "  [-0.41918583 -0.41917289 -0.42414809 -0.4214367 ]\n",
      "  [-0.41752396 -0.41928437 -0.41835979 -0.42107328]]\n",
      "\n",
      " [[-0.65536881 -0.65476612 -0.65393976 -0.65460947]\n",
      "  [-0.65553848 -0.65403727 -0.65420469 -0.65687617]\n",
      "  [-0.65695382 -0.65795907 -0.65655299 -0.65921574]\n",
      "  ...\n",
      "  [-0.42263919 -0.41860346 -0.41839105 -0.41992711]\n",
      "  [-0.41417169 -0.41735739 -0.41914772 -0.42317018]\n",
      "  [-0.41198862 -0.4127556  -0.41458455 -0.42130269]]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and MDP\n",
    "env = GridEnvironment()\n",
    "mdp = MDP(env)\n",
    "\n",
    "# Run Policy Iteration\n",
    "pi_agent = PolicyIterationAgent(mdp)\n",
    "pi_agent.policy_iteration()\n",
    "\n",
    "# Run Q-Learning\n",
    "ql_agent = QLearningAgent(mdp)\n",
    "ql_agent.train()\n",
    "\n",
    "# Compare results by examining value tables or testing optimal policies\n",
    "print(\"Policy Iteration Value Table:\", pi_agent.value_table)\n",
    "print(\"Q-Learning Q-Table:\", ql_agent.q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
